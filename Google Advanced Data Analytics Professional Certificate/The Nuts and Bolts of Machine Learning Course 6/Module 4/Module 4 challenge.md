# Module 4 challenge


### **Question 1**  
A data professional uses tree-based learning for an operations project. Currently, they are interested in the nodes at which the trees split. What type of nodes do they examine?

- **Answer:**  
  - **Decision**

---

### **Question 2**  
What are some disadvantages of decision trees? Select all that apply.

- **Answer:**
  - Decision trees can be particularly susceptible to overfitting.
  - Preparing data to train a decision tree is a complex process involving significant preprocessing.
  - When new data is introduced, decision trees can be less effective at prediction.

---

### **Question 3**  
Which section of a decision tree is where the final prediction is made?

- **Answer:**  
  - **Leaf node**

---

### **Question 4**  
In a decision tree ensemble model, which hyperparameter controls how many decision trees the model will build for its ensemble?

- **Answer:**  
  - **n_estimators**

---

### **Question 5**  
When might you use a separate validation dataset? Select all that apply.

- **Answer:**
  - If you want to compare different model scores to choose a champion before predicting on test holdout data.
  - If you have a very large amount of data.

---

### **Question 6**  
Which of the following statements correctly describe ensemble learning? Select all that apply.

- **Answer:**
  - It's possible to use the same methodology for each contributing model, as long as there are numerous base learners.
  - It's possible to use very different methodologies for each contributing model.
  - Ensemble learning involves building multiple models.

---

### **Question 7**  
Fill in the blank: A random forest model grows trees by taking a random subset of the available features in the training data, then _____ each node at the best feature available to that tree.

- **Answer:**  
  - **splitting**

---

### **Question 8**  
What are some benefits of boosting? Select all that apply.

- **Answer:**
  - Boosting can handle both numeric and categorical features.
  - Boosting is a powerful predictive methodology.
  - Boosting does not require the data to be scaled.

---

### **Question 9**  
Which of the following statements correctly describe gradient boosting? Select all that apply.

- **Answer:**
  - Gradient boosting machines do not give coefficients or directionality for their individual features.
  - Gradient boosting machines are often called black-box models because their predictions can be difficult to explain.
  - Gradient boosting machines have many hyperparameters.

---

### **Question 10**  
In tree-based models, what are Gini impurity, entropy, information gain, and log loss used for? Select all that apply.

- **Answer:**
  - To determine optimal split points of decision nodes.
  - To quantify purity/impurity of child nodes.

---

Let me know if you need further explanations!
